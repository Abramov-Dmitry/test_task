services:
  mlflow:
    container_name: "${MLFLOW_HOST}"
    image: ghcr.io/mlflow/mlflow
    user: 1000:1000
    ports:
    - "${MLFLOW_PORT:-5000}:5000/tcp"
    volumes:
      - ./mlflow/mlruns:/mlruns
    command: mlflow server --host 0.0.0.0

  classifier:
    container_name: "${CLASSIFIER_HOST}"
    build: ./classifier
    environment:
      - PYTHONPATH=/app
    ports:
      - "${CLASSIFIER_PORT:-5001}:5001/tcp"
    volumes:
      - ./mlflow:/mlflow
      - ./classifier/app:/app
      - ./helpers/:/app/helpers
      - .env:/app/.env:ro
    depends_on:
        - mlflow
    command: uvicorn service:app --host 0.0.0.0 --port 5001 --reload

  forecaster:
    build: ./temperature_forecasting
    container_name: "${FORECASTING_HOST}"
    environment:
      - PYTHONPATH=/app
    ports:
      - "${FORECASTING_PORT:-5002}:5002/tcp"
    volumes:
      - ./mlflow:/mlflow
      - ./temperature_forecasting/app:/app
      - ./helpers/:/app/helpers
      - .env:/app/.env:ro
    depends_on:
        - classifier
    command: uvicorn service:app --host 0.0.0.0 --port 5002 --reload

  degeneration:
    build: ./text_generation 
    container_name: "${GENERATION_HOST}"
    environment:
      - PYTHONPATH=/app
    ports:
      - "${GENERATION_PORT:-5003}:5003/tcp"
    volumes:
      - ./mlflow:/mlflow
      - ./text_generation/app:/app
      - ./helpers/:/app/helpers
      - .env:/app/.env:ro
    depends_on:
      - forecaster
    command: python3 service.py

  gradio:
    build: ./gradio_docker
    volumes:
      - ./gradio_docker/app:/app
      - .env:/app/.env
    depends_on:
        - degeneration
    ports:
      - "8000:8000/tcp"
    environment:
      - PYTHONPATH=/app     
    command: uvicorn gradio_server:app --host 0.0.0.0 --port 8000 --reload
